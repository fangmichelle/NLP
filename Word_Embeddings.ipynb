{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y5cwPLm6Lyw",
        "colab_type": "text"
      },
      "source": [
        "# Word Embeddings\n",
        "We will try to approximate a Skip-gram word embedding via positive pointwise mutual information (PPMI) and truncated singular value decomposition (SVD). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMnCOKC26Gzj",
        "colab_type": "text"
      },
      "source": [
        "## The setup\n",
        "Let's import the required libraries and load the data for preparing our word vectors. We are going to load a list of movie plot summaries (http://www.cs.cmu.edu/~ark/personas/) and use that as our corpus. You do not need to modify them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRKoyqtb0QL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code gets the data file from github and imports them into Colab\n",
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_3/plot_summaries_tokenized.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yWaVJn30NBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d7f77d38-6da5-4d15-9d45-6b19d832b145"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "from collections import Counter, defaultdict\n",
        "from math import log2\n",
        "from scipy.sparse import csc_matrix\n",
        "from scipy.sparse.linalg import svds\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loads the data and returns tokenized summaries.\n",
        "    \n",
        "    :return summaries_tokenized: a list that contains tokenized summaries text\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(\"plot_summaries_tokenized.csv\")\n",
        "    summaries_tokenized = list(df['SUMMARY'].apply(lambda text: text.split()))\n",
        "    return summaries_tokenized\n",
        "\n",
        "summaries = load_data()\n",
        "num_summaries = len(summaries)\n",
        "print(\"There are {} summaries.\".format(num_summaries))\n",
        "print(\"Example tokenized summary:\", summaries[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 42303 summaries.\n",
            "Example tokenized summary: ['Shlykov', 'a', 'hardworking', 'taxi', 'driver', 'and', 'Lyosha', 'a', 'saxophonist', 'develop', 'a', 'bizarre', 'lovehate', 'relationship', 'and', 'despite', 'their', 'prejudices', 'realize', 'they', 'arent', 'so', 'different', 'after', 'all']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ikv9DyqR7xoG",
        "colab_type": "text"
      },
      "source": [
        "We have ~42000 summaries containing ~13000000 words. We will now proceed by creating a vocabulary and will limit its size to something computationally feasible. You may find python's collections.Counter function useful. You may not import any additional libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWP4hmGG7--v",
        "colab_type": "text"
      },
      "source": [
        "# 1. Create Vocabulary\n",
        "We will start from creating our vocabulary. Vocabulary contains unigrams and their counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksw96WHvEoJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################\n",
        "# Do not modify\n",
        "###################\n",
        "min_count = (1 / 100) * len(summaries)\n",
        "max_count = (1 / 10) * len(summaries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sum1rnZN54-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def creat_vocabulary(tokenized_documents, min_count, max_count):\n",
        "    \"\"\"\n",
        "    This function takes in tokenized documents and returns a\n",
        "    vocabulary and word <-> index lookup dictionary of some frequently appearing words.\n",
        "    \n",
        "    :param tokenized_documents: a list of tokenized strings\n",
        "    :param min_count: minimum unigram count\n",
        "    :param max_count: maximum unigram count\n",
        "    :return vocab: a Counter where vocab[word] = count of word's occurences in all the documents\n",
        "    :return word2idx: a word -> index lookup Dictionary for words in vocab.\n",
        "    :return idx2word: a index -> word lookup Dictionary for words in vocab.\n",
        "    \"\"\"\n",
        "    # 1a. Compute unigram counts. A unigram is a single word, e.g. foo\n",
        "    vocab = Counter()\n",
        "    ##################################\n",
        "    for sentence in tokenized_documents:\n",
        "      for i in sentence:\n",
        "        if i not in vocab.keys():\n",
        "          vocab[i] = 1\n",
        "        else:\n",
        "          vocab[i] += 1\n",
        "    ##################################\n",
        "      \n",
        "    # 1b. Remove unigrams that has #(unigram) < min_count or #(unigram) > max_count\n",
        "    # to eliminate unigrams occurring very frequently or infrequently. \n",
        "    # This will limit its size to something computationally feasible.\n",
        "    print('%d vocabs before' % len(vocab))\n",
        "    ##################################\n",
        "    tbd = []\n",
        "    for v in vocab:\n",
        "      if vocab[v] < min_count or vocab[v] > max_count:\n",
        "        tbd.append(v)\n",
        "    \n",
        "    for t in tbd:\n",
        "      del vocab[t]\n",
        "    ##################################\n",
        "    print('%d vocabs after' % len(vocab))\n",
        "          \n",
        "    # 1c. Build word <-> index lookup for words in vocab.\n",
        "    word2idx, idx2word = {}, {}\n",
        "    ##################################\n",
        "    idx = 0\n",
        "    for word in vocab:\n",
        "      word2idx[word] = idx\n",
        "      idx2word[idx] = word\n",
        "      idx += 1\n",
        "\n",
        "    #for sentence in tokenized_documents:\n",
        "      #for word in sentence:\n",
        "        #if word in vocab.keys():\n",
        "          #word2idx[word] = sentence.index(word)\n",
        "          #idx2word[sentence.index(word)] = word\n",
        "    ##################################\n",
        "    \n",
        "    return vocab, word2idx, idx2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71G0q8l_51CH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "420fc184-d4f2-4f50-f269-2d44491fd6dd"
      },
      "source": [
        "vocab, word2idx, idx2word = creat_vocabulary(summaries, min_count, max_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "214147 vocabs before\n",
            "2750 vocabs after\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NwfhahT_tRd",
        "colab_type": "text"
      },
      "source": [
        "# 2. Build Term-Context Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ-tvqGE1ykI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################\n",
        "# Do not modify\n",
        "###################\n",
        "window_size = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W-kjSaKUJ_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def put_in_dict(dic, val):\n",
        "  if val not in dic.keys():\n",
        "    dic[val] = 1\n",
        "  else:\n",
        "    dic[val] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQvXB-MZ_VqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_term_context_matrix(tokenized_documents, vocab, window_size):\n",
        "    \"\"\"\n",
        "    This function returns a `word_pair_count` Counter with each \n",
        "    word_pair_count[(w, c)] = number of times the word `c` occurs in the context of word `w`. (where `w`, `c` belong to the vocab)\n",
        "    To make it efficient, instead of building the sparse term-context matrix, \n",
        "    we will build 3 separate Counters: word_pair_count, w_count, c_count\n",
        "    You may find python's Counter useful here\n",
        "\n",
        "    :param tokenized_documents: a list of tokenized strings\n",
        "    :param vocab: vocabulary Counter\n",
        "    :param window_size: context window size\n",
        "    :return word_pair_count: a Counter where word_pair_count[(w, c)] = count of c's occurences in w's context window, i.e. #(w, c)\n",
        "    :return w_count: a Counter where w_count[w] = the number of times w occured in the documents, i.e. #(w)\n",
        "    :return c_count: a Counter where c_count[c] = the number of times c occured in the documents, i.e. #(c)\n",
        "    \"\"\"\n",
        "    word_pair_count = Counter()  \n",
        "    w_count = Counter()\n",
        "    c_count = Counter()\n",
        "    ##################################\n",
        "\n",
        "    for sentence in tokenized_documents:\n",
        "      for idx, word in enumerate(sentence):\n",
        "        if word in vocab.keys():\n",
        "          # make context window\n",
        "          mid = idx\n",
        "          # if mid + window_size > len(sentence):\n",
        "            # end = sentence[mid+1:len(sentence)]\n",
        "          # else:\n",
        "            # end = sentence[mid+1:mid+window_size]\n",
        "          context = sentence[max(0, mid-window_size):mid] + sentence[mid+1:mid+window_size+1] # end\n",
        "          for c in context:\n",
        "            if c in vocab.keys():\n",
        "              # if c in V\n",
        "              c_count[c] += 1\n",
        "              w_count[word] += 1\n",
        "              # add to word_pair_count\n",
        "              #put_in_dict(word_pair_count, (word, c))\n",
        "              word_pair_count[(word, c)] += 1\n",
        "\n",
        "    ##################################\n",
        "    \n",
        "    return word_pair_count, w_count, c_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBxi0t1y2jQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d84bc3d1-2df0-43d2-e0f4-4a24f38b1d64"
      },
      "source": [
        "word_pair_count, w_count, c_count = build_term_context_matrix(summaries, vocab, window_size)\n",
        "print(\"There are {} word-context pairs\".format(len(word_pair_count)))\n",
        "\n",
        "# The number of w_count and c_count should match your number of vocab\n",
        "assert len(w_count) == len(vocab)\n",
        "assert len(c_count) == len(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1917545 word-context pairs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeaZoasi3m5r",
        "colab_type": "text"
      },
      "source": [
        "# 3. Build Positive Pointwise Mutual Information (PPMI) Matrix\n",
        "In this part, you will build a PPMI matrix using Scipy's Compressed Sparse Column matrix to save storage space. (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)\n",
        "\n",
        "Sparse matrix is a matrix which contains very few non-zero elements. When a sparse matrix is represented with a 2-dimensional array, we waste a lot of space to represent that matrix. In NLP application, it's quite common to use sparse matrix since the size of vocabulary is usually very large. \n",
        "\n",
        "Below is an example of how to build a sparse matrix where `data`, `row` and `col` satisfy the relationship `M[row[k], col[k]] = data[k]`.\n",
        "\n",
        "```python\n",
        ">>> row = np.array([0, 2, 2, 0, 1, 2])\n",
        ">>> col = np.array([0, 0, 1, 2, 2, 2])\n",
        ">>> data = np.array([1, 2, 3, 4, 5, 6])\n",
        ">>> M = csc_matrix((data, (row, col)))\n",
        ">>> M.toarray()\n",
        "array([[1, 0, 4],\n",
        "       [0, 0, 5],\n",
        "       [2, 3, 6]])\n",
        "```\n",
        "\n",
        "Recall that\n",
        "$$\n",
        "\\begin{gather*}\n",
        "  \\text{PMI}(w, c) = \\log_2 \\frac{P(w, c)}{P(w)P(c)} \\\\\n",
        "  \\text{PPMI}(w, c) = \\max(0, \\text{PMI}(w, c))\n",
        "\\end{gather*}\n",
        "$$\n",
        "You should use `log2` function from the math package that is alreadly imported for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIYharDm38G1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_PPMI_matrix(word_pair_count, w_count, c_count, word2idx):\n",
        "    \"\"\"\n",
        "    This function returns a PPMI matrix represented by a csc sparse matrix.\n",
        "\n",
        "    :params word_pair_count: a Counter where word_pair_count[(w, c)] = count of c's occurences in w's context window\n",
        "    :params w_count: a Counter where w_count[w] = the number of times w occured in the documents\n",
        "    :params c_count: a Counter where c_count[c] = the number of times c occured in the documents\n",
        "    :params word2idx: a word -> index lookup Dictionary for words in vocab\n",
        "    :return PPMI: PPMI csc sparse matrix\n",
        "    \"\"\"\n",
        "    data, rows, cols = [], [], []\n",
        "    total_occurences = sum(word_pair_count.values())\n",
        "    for (w, c), n in word_pair_count.items():\n",
        "      ##################################\n",
        "      w = (w, c)[0]\n",
        "      c = (w, c)[1]\n",
        "      p_wc = n / total_occurences\n",
        "      p_w = w_count[w] / total_occurences\n",
        "      p_c = c_count[c] / total_occurences\n",
        "      val = max(0, log2(p_wc/(p_w*p_c)))\n",
        "      data.append(val)\n",
        "      rows.append(word2idx[w])\n",
        "      cols.append(word2idx[c])\n",
        "      # rows = w, cols = c\n",
        "      ##################################\n",
        "\n",
        "    PPMI = csc_matrix((data, (rows, cols)))\n",
        "    return PPMI"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADuP5FPV8-XQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PPMI = build_PPMI_matrix(word_pair_count, w_count, c_count, word2idx)\n",
        "\n",
        "# The shape of PPMI matrix should match your number of vocab\n",
        "assert PPMI.shape == (len(vocab), len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLUHCDzN9PGF",
        "colab_type": "text"
      },
      "source": [
        "# 4. Truncated SVD\n",
        "In this part, we will obtain a dense low-dimensional vectors via truncated (rank-k) SVD. You should use `svds` function from Sicpy that is already imported for you to obtain the SVD factorization.\n",
        "(https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEh5rynC9-UR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################\n",
        "# Do not modify\n",
        "###################\n",
        "rank = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLtCNz5Z9U8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embeddings(PPMI, rank):\n",
        "    \"\"\"\n",
        "    Reutrns the left singular vectors as word embeddings via truncated SVD\n",
        "\n",
        "    :params PPMI: PPMI csc sparse matrix\n",
        "    :params rank: number of singular values and vectors to compute\n",
        "    :return u: left sigular vectors from sprase SVD\n",
        "    :return s: the singular values from sparse SVD\n",
        "    \"\"\"\n",
        "    ##################################\n",
        "    u,s,vt = svds(PPMI, k=rank)\n",
        "    ##################################\n",
        "\n",
        "    return u, s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmjoP5KF91O0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings, _ = get_embeddings(PPMI, rank)\n",
        "embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)  # Normalize embeddings matrix\n",
        "\n",
        "# The shape of the embeddings matrix should be (# vocab, rank)\n",
        "assert embeddings.shape == (len(vocab), rank)\n",
        "\n",
        "# Make sure embeddings is normalized\n",
        "assert True == np.isclose(np.linalg.norm(embeddings[0]), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQUUfS0N-Lyc",
        "colab_type": "text"
      },
      "source": [
        "# 5. Evaluate Word Embeddings via Cosine Similarity\n",
        "\n",
        "Using cosine similarity as a measure of distance [§6.4 Jurafsky & Martin](https://web.stanford.edu/~jurafsky/slp3/6.pdf), we will now find the closest words to a certain word. We define cosine similarity as, $$cosine(\\overrightarrow{v},\\overrightarrow{w}) = \\frac{\\overrightarrow{v} \\cdot \\overrightarrow{w}}{\\vert v \\vert \\vert w \\vert}$$\n",
        "\n",
        "Please complete the function below that calculates the 'K' closest words from the vocabulary. You may not use any additional libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9Zf_us2AFkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################\n",
        "# Do not modify\n",
        "###################\n",
        "num_neighbors = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_l55j98-NvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_distances(matrix, vector):\n",
        "    \"\"\"\n",
        "    The function takes in a matrix and a vector (both normalized) \n",
        "    and returns the cosine distances for this vector against all others.\n",
        "    The pretrained embeddings are normalized.\n",
        "\n",
        "    :params matrix: word embeddings matrix\n",
        "    :params vector: word vector for a particular word\n",
        "    :return distances: a cosine distances vector\n",
        "    \"\"\"\n",
        "    ##################################\n",
        "    distances = np.dot(matrix, vector)\n",
        "    ##################################\n",
        "    \n",
        "    return  distances\n",
        "\n",
        "\n",
        "def nearest_neighbors(embeddings, word, k, word2idx, idx2word):\n",
        "    \"\"\"\n",
        "    For each query word, this function returns the k closest words from the vocabulary.\n",
        "\n",
        "    :params embeddings: word embedding matrix\n",
        "    :params word: query word\n",
        "    :params k: number of cloest words to return\n",
        "    :params word2idx: a word -> index lookup dictionary\n",
        "    :params idx2word: a index -> word lookup dictionary\n",
        "    :return nearest_neighbors: a list of cloest words\n",
        "    \"\"\"\n",
        "    vector = embeddings[word2idx[word]]\n",
        "    distances = cosine_distances(embeddings, vector)\n",
        "    nearest_neighbors = []\n",
        "    ##################################\n",
        "    sorted_dist = list(reversed(np.argsort(distances)))\n",
        "    for i in sorted_dist[1:k+1]:\n",
        "      nearest_neighbors.append(idx2word[i])\n",
        "    # argsort -> take top k :: returns indices of top k\n",
        "    # find top k distances\n",
        "    # use idx2word to find top k words?\n",
        "    ##################################\n",
        "    \n",
        "    return nearest_neighbors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJjPuVPe_oGq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9c7f2596-7bef-4d27-f1b6-04f308b66eae"
      },
      "source": [
        "query_words = [\"doctor\", \"zombie\", \"robot\", \"eat\", \"bus\"]\n",
        "for word in query_words:\n",
        "    print(word, nearest_neighbors(embeddings, word, num_neighbors, word2idx, idx2word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doctor ['Sophie', 'priest', 'doctors', 'Christine', 'Anna']\n",
            "zombie ['infected', 'zombies', 'vampires', 'creatures', 'vampire']\n",
            "robot ['alien', 'creature', 'weapon', 'machine', 'demon']\n",
            "eat ['throw', 'sleep', 'wear', 'stand', 'sit']\n",
            "bus ['road', 'boat', 'truck', 'airport', 'river']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfxaNuBjAiiY",
        "colab_type": "text"
      },
      "source": [
        "# 6. Evaluate Word Embeddings via Analogous Tasks\n",
        "\n",
        "The embedding space is known to capture the semantic context of words. An example of it is $\\overrightarrow{woman} - \\overrightarrow{man} \\simeq \\overrightarrow{queen} - \\overrightarrow{king}$. Use the `cosine_distances()` function you wrote above to find such relations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZQCzP-FCRb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relation(embeddings, query_words, word2idx, idx2word):\n",
        "    \"\"\"\n",
        "    Takes in 3 words and returns the closest word (in terms of cosine similarity)\n",
        "    to the normalized algebraic addition of the three vectors.\n",
        "    The parameters follow this order : word_vec1 - word_vec2 ~ closest - word_vec3\n",
        "\n",
        "    :params embeddings: word embedding matrix\n",
        "    :params query_words: a list of query words in the following order: [word1, word2, word3]\n",
        "    :params word2idx: a word -> index lookup dictionary\n",
        "    :params idx2word: a index -> word lookup dictionary\n",
        "    :return closet_word: the closest word for the relation\n",
        "    \"\"\"\n",
        "    word1, word2, word3 = query_words\n",
        "    if all(word in vocab for word in query_words):\n",
        "      ##################################\n",
        "      # closest vec = vec1 - vec2 + vec3\n",
        "      # normalize closest vec\n",
        "      # use cosine distances to find word in embeddings closest to closest vec\n",
        "      closest_vec = embeddings[word2idx[word1]] - embeddings[word2idx[word2]] + embeddings[word2idx[word3]]\n",
        "      closest_vec /= np.linalg.norm(closest_vec) #, keepdims=True)\n",
        "      sorted_args = list(reversed(np.argsort(cosine_distances(embeddings, closest_vec))))\n",
        "      #if idx2word[sorted_args[0]] in query_words:\n",
        "        #closet_word = idx2word[sorted_args[1]]\n",
        "      #else:\n",
        "        #closet_word = idx2word[sorted_args[0]]\n",
        "      closet_word = idx2word[sorted_args[1]]\n",
        "      ##################################\n",
        "      \n",
        "      return closet_word\n",
        "    else:\n",
        "      missing = [w for w in query_words if w not in vocab]\n",
        "      raise Exception(\"missing {} from vocabulary\".format(\", \".join(missing)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF3mtHMjHue-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d7bc3f16-544f-40c5-99b6-565cf7681cec"
      },
      "source": [
        "queries = [[\"doctor\", \"nurse\", \"king\"], [\"robot\", \"weapon\", \"bus\"], [\"sing\", \"song\", \"justice\"], [\"elderly\", \"kids\", \"teenager\"], [\"soldier\", \"wound\", \"telephone\"]]\n",
        "for query in queries:\n",
        "  closet_word = relation(embeddings, query, word2idx, idx2word)\n",
        "  print(\"{} - {} ~= {} - {}\".format(query[0], query[1], closet_word, query[2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doctor - nurse ~= Emperor - king\n",
            "robot - weapon ~= road - bus\n",
            "sing - song ~= defend - justice\n",
            "elderly - kids ~= elderly - teenager\n",
            "soldier - wound ~= newspaper - telephone\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}